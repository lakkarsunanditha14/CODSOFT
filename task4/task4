import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

# Load pretrained VGG16 model
vgg_model = VGG16(weights="imagenet")
vgg_model = tf.keras.Model(
    inputs=vgg_model.inputs,
    outputs=vgg_model.layers[-2].output
)

def extract_features(image_path):
    image = load_img(image_path, target_size=(224, 224))
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    image = preprocess_input(image)
    
    features = vgg_model.predict(image, verbose=0)
    return features
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add

vocab_size = 5000
max_length = 34

# Image feature branch
image_input = Input(shape=(4096,))
image_dense = Dense(256, activation='relu')(image_input)

# Text branch
caption_input = Input(shape=(max_length,))
caption_embedding = Embedding(vocab_size, 256, mask_zero=True)(caption_input)
caption_lstm = LSTM(256)(caption_embedding)

# Combine both
decoder = add([image_dense, caption_lstm])
decoder_dense = Dense(256, activation='relu')(decoder)
output = Dense(vocab_size, activation='softmax')(decoder_dense)

# Model
model = Model(inputs=[image_input, caption_input], outputs=output)
model.compile(loss='categorical_crossentropy', optimizer='adam')

model.summary()
def generate_caption(model, tokenizer, photo_feature, max_length):
    caption = 'startseq'
    
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([caption])[0]
        sequence = tf.keras.preprocessing.sequence.pad_sequences(
            [sequence], maxlen=max_length
        )
        
        yhat = model.predict([photo_feature, sequence], verbose=0)
        yhat = np.argmax(yhat)
        
        word = tokenizer.index_word.get(yhat)
        if word is None:
            break
        
        caption += ' ' + word
        if word == 'endseq':
            break
            
    return caption
image_path = "images/sample.jpg"
photo = extract_features(image_path)

# tokenizer must be trained on captions dataset
caption = generate_caption(model, tokenizer, photo, max_length)
print("Generated Caption:", caption)
